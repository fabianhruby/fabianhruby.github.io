<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
    <meta property="og:type" content="article">
    <meta property="og:title" content="Preprocessing with Scikit-learn Pipelines (2/3)">
    <meta property="og:url"
        content="https://fabianhruby.github.io/posts/2024/preprocessing/preprocessing-pipelines-part2.html">
    <meta property="og:image"
        content="https://fabianhruby.github.io/assets/img/posts/2024/preprocessing/preprocessing-with-scikit-learn-pipelines-part2.jpg">
    <meta property="og:description"
        content="This article dives into building custom transformers to preprocess categorical data in scikit-learn pipelines. It covers techniques for removing special characters, binning postal codes, and one-hot encoding categorical features. You'll also learn how to integrate custom transformers into your preprocessing pipeline for a streamlined workflow.">
    <meta property="article:author" content="Fabian Hruby">
    <meta property="article:tag" content="data science">
    <meta property="article:tag" content="scikit-learn">
    <meta property="article:tag" content="sklearn">
    <meta property="article:tag" content="data preprocessing">
    <meta property="article:tag" content="machine learning">
    <title>Preprocessing with scikit-learn Pipelines | Part 2 - Byte-Sized Insights</title>
    <link rel="icon" type="image/x-icon" href="/assets/favicon.ico" />
    <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
    <!-- Google fonts-->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link
        href="https://fonts.googleapis.com/css2?family=Fira+Sans:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&display=swap"
        rel="stylesheet">
    <!-- Core theme CSS (includes Bootstrap)-->
    <link href="/css/styles.css" rel="stylesheet" />
    <link href="/css/prism-atom-dark.css" rel="stylesheet" />
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
        MathJax.Hub.Config({messageStyle: 'none',tex2jax: {preview: 'none'}});
      </script>
    <script type="text/javascript" charset="utf-8" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,
    https://vincenttam.github.io/javascripts/MathJaxLocal.js"></script>

</head>

<body>
    <!-- Navigation-->
    <nav class="navbar navbar-expand-lg navbar-light" id="mainNav">
        <div class="container px-4 px-lg-5">
            <a class="navbar-brand" href="/index.html">Byte-Sized Insights</a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive"
                aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
                Menu
                <i class="fas fa-bars"></i>
            </button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav ms-auto py-4 py-lg-0">
                    <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="/index.html">Home</a></li>
                    <li class="nav-item"><a class="nav-link px-lg-3 py-3 py-lg-4" href="/about.html">About</a></li>
                    </li>
                </ul>
            </div>
        </div>
    </nav>
    <!-- Page Header-->
    <header class="masthead"
        style="background-image: url('/assets/img/posts/2024/preprocessing/preprocessing-with-scikit-learn-pipelines-part2.jpg')">
        <div class="container position-relative px-4 px-lg-5">
            <div class="row gx-4 gx-lg-5 justify-content-center">
                <div class="col-md-6 col-lg-8 col-xl-8">
                    <div class="post-heading">
                        <h1>Preprocessing with Scikit-learn Pipelines (2/3)</h1>
                        <h2 class="subheading">Elevate Your Preprocessing with Custom Transformers</h2>
                        <span class="meta">
                            Posted by Fabian Hruby on June 12, 2024
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </header>
    <!-- Post Content-->
    <article class="mb-4">
        <div class="container px-4 px-lg-5">
            <div class="row gx-4 gx-lg-6 justify-content-center">
                <div class="col-md-10 col-lg-10 col-xl-8">
                    <h2>Introduction</h2>
                    <p>Building on our understanding of data preprocessing with
                        the Scikit-learn <incode>Pipeline</incode>
                        from the <a href="/posts/2024/preprocessing/preprocessing-pipelines-part1.html">first part of
                            this
                            series</a>, let's explore transforming categorical data. This part goes beyond basic
                        techniques by introducing custom transformers for tailored data cleaning. Our introductory
                        example
                        was straightforward, focusing on missing numerical values. However, handling categorical
                        variables can be more complex, especially when replacing missing values and cleaning messy data.
                    </p>
                    <p>In this part of the series, we will look at transforming categorical data using scikit-learn's
                        <incode>FunctionTransformer</incode> and explore the use of custom transformers to define
                        specific preprocessing functions tailored to your data.
                    </p>
                    <h2>Setting the Goals for Our Next Transformations</h2>
                    <p>Speaking of data, let's take a look at our data after applying the transformations from part 1:
                        <br><br>
                        <img class="img-fluid"
                            src="/assets/img/posts/2024/preprocessing/preprocessing-with-scikit-learn-pipelines-part2-data.png">
                        <br><br>
                        Our columns <incode>age</incode>, <incode>experience_in_years</incode> and <incode>
                            salary_in_dollar</incode> look well prepared after we imputed missing values and scaled them
                        with scikit-learn's <incode>MinMaxScaler</incode>.
                        <br>However, the highlighted values in the columns <incode>job_title</incode> and <incode>city
                        </incode>
                        need attention.
                    </p>
                    <p>For example, models such as neural networks only work with numerical data, so we need to convert
                        our categorical
                        data, such as <incode>job_title</incode> and <incode>city</incode>, into numbers. Although
                        <incode>postal_codes</incode> are already integers, they represent categories, so we will bin
                        these values into
                        groups before applying <a
                            href="https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics">one-hot
                            encoding</a>. This approach will reduce the dimensionality of the feature space while
                        retaining meaningful information.
                    </p>
                    <p>However, before converting the strings to numbers and binning our postal codes, we must address
                        the special characters in the <incode>job_title</incode> column. Ignoring these characters would
                        result in treating "Data A*nalyst" and "Data Analyst" as different strings, which is
                        undesirable. Therefore, we need to clean up the data in the <incode>job_title</incode> column.
                    </p>
                    <p>So here are our preprocessing goals:
                    <ul>
                        <li>Remove special characters from the <incode>job_title</incode> column</li>
                        <li>Create bins and assign each postal code to a bin</li>
                        <li>One-hot encode the <incode>job_title</incode>, <incode>city</incode>, and <incode>
                                postal_code_bin</incode> columns (created in the previous step)
                        </li>
                    </ul>
                    </p>
                    <h2>Applying FunctionTransformer and Custom Transformer</h2>
                    To clean the <incode>job_title</incode> column and bin the values in the
                    <incode>postal_codes</incode> column, we will leverage scikit-learn's
                    <incode>FunctionTransformer</incode> and build our
                    own custom transformer, respectively. Incorporating these transformations within our preprocessing
                    pipeline allows us to streamline the data preprocessing steps. As the blog title says, we'll be
                    using
                    some tools from the scikit-learn library, although this is not the most robust solution as we won't
                    be using the <incode>ColumnTransformer</incode>. Stay tuned for part three of this series where
                    we'll introduce the <incode>ColumnTransformer</incode> for a more elegant approach.
                    </p>
                    <h3>Using Scikit-learn's FunctionTransformer</h3>
                    Here is the complete script, some of which you may already know from the <a
                        href="/posts/2024/preprocessing/preprocessing-pipelines-part1.html">first part of
                        this
                        series</a>:
                    <br>
                    <pre>
<code class="language-python">"""Script for preprocessing data"""

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer, MinMaxScaler

from preprocessing import (
    PostalCodeBinTransformer,
    one_hot_encode_columns,
    remove_special_chars,
)

# Path to our raw data
DATA_PATH = "data/data.xlsx"

# Reading in data and converting the postal_code column as an integer
raw_data = pd.read_excel(DATA_PATH, converters={"postal_code": int})

# Selecting the columns in which missing values should be imputed
cols_to_impute = ["age", "experience_in_years", "salary_in_dollar"]

# Creating our Pipeline object with a SimpleImputer and MinMaxScaler from sklearn
pipeline_num = Pipeline(
    [
        ("MeanImputer", SimpleImputer(missing_values=np.nan, strategy="mean")),
        ("MinMaxScaler", MinMaxScaler()),
    ]
)

# Creating our special_char_remover with scikit-learn's FunctionTransformer
special_char_remover = FunctionTransformer(
    remove_special_chars,
    kw_args={"column_name": "job_title"}
).set_output(transform="pandas")

# Name the columns which should be one hot encoded
columns_to_encode = ["job_title", "city", "postal_code_bin"]

# Creating our one hot encoding with scikit-learn's FunctionTransformer
one_hot_encoder = FunctionTransformer(
    one_hot_encode_columns,
    kw_args={"columns_to_encode": columns_to_encode},
)

# Creating our second Pipeline object with our
# custom preprocessing function and our CustomTransformers
pipeline_cat = Pipeline(
    [
        ("SpecialCharRemover", special_char_remover),
        ("PostalCodeBinner", PostalCodeBinTransformer()),
        ("OneHotEncoder", one_hot_encoder),
    ]
)

# Coping our raw_data and imputing missing values with the column's mean
imputed_data = raw_data.copy()
imputed_data[cols_to_impute] = pipeline_num.fit_transform(raw_data[cols_to_impute])

# Remove the special characters from the job_title column
# Add column postal_code_bin based on the postal_code column
# One-not encode the columns in the list columns_to_encode
cleaned_data = pipeline_cat.fit_transform(imputed_data)</code>
</pre>
                    <p>And here is the code from our imported functions and our custom transfomer from the <incode>
                            preprocessing.py</incode> file:</p>

                    <pre>
    <code class="language-python">
import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin


def remove_special_chars(df: pd.DataFrame, column_name: str) -> pd.Series:
    """
    Removes special characters from a column.

    Parameters
    ----------
    df : pd.DataFrame
        The DataFrame containing the column to be cleaned.
    col_name : str
        The name of the column which values should be cleaned from special characters.

    Returns
    -------
    pd.Series
        The column with its values cleaned from special characters.
    """
    # Define pattern for special characters
    pattern = r"[\*\/!_\-.,;?$%&$\^°]"
    df[column_name] = df[column_name].str.replace(pattern, "", regex=True)
    return df


def one_hot_encode_columns(df: pd.DataFrame, columns_to_encode: list[str])
-> pd.DataFrame:
    """
    One-hot encodes specified categorical columns in a DataFrame.

    Parameters
    ----------
    df: pd.DataFrame or np.ndarray
        The input DataFrame or array containing categorical columns to be
        one-hot encoded.

    columns_to_encode : list[str]
        A list of column names in the DataFrame to be one-hot encoded.

    Returns
    -------
    pd.DataFrame or np.ndarray
        DataFrame or array with one-hot encoded columns.
    """
    if isinstance(df, pd.DataFrame):
        encoded_df = pd.get_dummies(df, columns=columns_to_encode)
        return encoded_df
    elif isinstance(df, np.ndarray):
        raise ValueError("Input must be a DataFrame for one-hot encoding.")
    else:
        raise TypeError("Unsupported input type. Expected DataFrame or numpy array.")


class PostalCodeBinTransformer(BaseEstimator, TransformerMixin):
    def __init__(self):
    self.bins = [
        0, 10000, 20000, 30000, 40000,
        50000, 60000, 70000, 80000, 90000, 100000,
    ]
    self.labels = [
        "0-10000", "10000-20000", "20000-30000", "30000-40000",
        "40000-50000", "50000-60000", "60000-70000", "70000-80000",
        "80000-90000", "90000-100000",
    ]

    def fit(self, X: pd.DataFrame):
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        postal_codes = X["postal_code"].astype(int)
        X["postal_code_bin"] = pd.cut(postal_codes,
                                      bins=self.bins,
                                      labels=self.labels
                                )
        return X</code>
</pre>
                    <p>The following step-by-step explanation will focus on how we use scikit-learn's <incode>
                            FunctionTransformer</incode>
                        to integrate our <incode>remove_special_chars</incode> and <incode>one_hot_encode_columns
                        </incode>
                        functions into the pipeline. We will also use our custom <incode>
                            PostalCodeBinTransformer</incode>
                        to bin the values of the <incode>postal_code</incode> column into groups:</p>
                    <h3>Imputing and Scaling Numerical Data</h3>
                    <p></p>
                    <pre><code class="language-python">"""Script for preprocessing data"""

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer, MinMaxScaler

from preprocessing import remove_special_chars

# Path to our raw data
DATA_PATH = "data/data.xlsx"

# Reading in data and converting the postal_code column as an integer
raw_data = pd.read_excel(DATA_PATH, converters={"postal_code": int})

# Selecting the columns in which missing values should be imputed
cols_to_impute = ["age", "experience_in_years", "salary_in_dollar"]

# Creating our Pipeline object with a SimpleImputer and MinMaxScaler from sklearn
pipeline_num = Pipeline(
    [
        ("MeanImputer", SimpleImputer(missing_values=np.nan, strategy="mean")),
        ("MinMaxScaler", MinMaxScaler()),
    ]
)</code></pre>
                    <p>The first part of the script preprocesses the numeric data. It imputes missing
                        values in the <incode>age</incode>, <incode>experience_in_years</incode> and <incode>
                            salary_in_dollar</incode> columns with the mean value of each column and then scales the
                        data between 0 and
                        1 using a MinMaxScaler. Refer to the <a
                            href="/posts/2024/preprocessing/preprocessing-pipelines-part1.html">first part of
                            this
                            series</a> for more details.</p>
                    <h3>Transforming our Functions with Scikit-learn's FunctionTransformer</h3>

                    <incode>remove_special_chars</incode>
                    <p></p>


                    <pre><code class="language-python"># Selecting the column from which special characters should be removed
special_char_remover = FunctionTransformer(
    remove_special_chars,
    kw_args={"column_name": "job_title"}
).set_output(transform="pandas")</code></pre>


                    <li>
                        <incode>FunctionTransformer</incode> allows us to apply custom functions to data
                        within
                        a
                        pipeline. In this case, we are transforming our imported <incode>
                            remove_special_chars</incode>
                        function.
                    </li>
                    <li>
                        <incode>remove_special_cars</incode> refers to the earlier imported function that
                        removes
                        special characters from a DataFrame column.
                    </li>
                    <li>The argument <incode>kw_args={"column_name": "job_title"}</incode> allows us to
                        specify
                        keyword
                        arguments – in this case we want to set the <incode>column_name</incode> argument to
                        "job_title"
                        – for our custom <incode>remove_special_chars</incode> function to ensure the right
                        column
                        is
                        cleaned.</li>
                    <li>The <incode>.set_output(transform="pandas")</incode> method is used to specify the
                        output type of the transformer. By default, Scikit-learn transformers return NumPy
                        arrays. However, when working with DataFrames, it's often more convenient to
                        maintain the DataFrame format to preserve column names and other metadata. This is
                        especially useful when you need to apply further transformations or inspect the
                        transformed data.</li>
                    <p>In summary, this code creates a resuable transformer based on a custom function that
                        can
                        easily be included in a scikit-learn pipeline to clean special characters from a
                        specific
                        column.</p>
                    </ul>


                    <incode>one_hot_encode_columns</incode>

                    <p></p>

                    <pre><code class="language-python"># Name the columns which should be one hot encoded
columns_to_encode = ["job_title", "city", "postal_code_bin"]

# Creating our one hot encoding with scikit-learn's FunctionTransformer
one_hot_encoder = FunctionTransformer(
    one_hot_encode_columns,
    kw_args={"columns_to_encode": columns_to_encode},
)</code>
</pre>
                    <li>Similarly, we use <incode>FunctionTransformer</incode> to convert our <incode>
                            one_hot_encode_columns</incode> function into a transformer for one-hot encoding of
                        specified categorical columns.</li>
                    </ul>
                    <p></p>
                    <h3>Building the Categorical Data Preprocessing Pipeline</h3>
                    <p>Let's now move on to creating the second pipeline that handles the categorical data. This
                        pipeline contains our custom preprocessing functions and transformers to ensure that our
                        categorical data is effectively cleaned, binned and one-hot encoded. <br>Here is the code to
                        create this pipeline:
                    <pre><code class="language-python"># Creating our second Pipeline object with our custom preprocessing function
# and our CustomTransformers
pipeline_cat = Pipeline(
    [
        ("SpecialCharRemover", special_char_remover),
        ("PostalCodeBinner", PostalCodeBinTransformer()),
        ("OneHotEncoder", one_hot_encoder),
    ]
)
</code></pre>
                    <p></p>
                    <h4>
                        1. Removing Special Characters from the job_title column
                    </h4>
                    <p></p>
                    <pre><code class="language-python">("SpecialCharRemover", special_char_remover)
</code></pre>
                    <p>This step uses our transformer <incode>special_char_remover</incode>, which cleans the
                        <incode>job_title</incode> column by removing unwanted special characters. This ensures
                        that
                        variations in job titles caused by special characters do not lead to redundant
                        categories.
                    </p>
                    <h4>
                        2. Creating Bins for Postal Codes
                    </h4>
                    <p></p>
                    <pre><code class="language-python">("PostalCodeBinner", PostalCodeBinTransformer())
</code></pre>
                    <p>This custom transformer, <incode>PostalCodeBinTransformer</incode>, bins postal codes
                        into predefined groups. This reduces the high-dimensional feature space that would
                        result from one-hot encoding individual postal codes. Let's look at how this custom
                        transformer is implemented:</p>
                    <pre><code class="language-python">class PostalCodeBinTransformer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.bins = [
            0, 10000, 20000, 30000, 40000,
            50000, 60000, 70000, 80000, 90000, 100000,
        ]
        self.labels = [
            "0-10000", "10000-20000", "20000-30000", "30000-40000",
            "40000-50000", "50000-60000", "60000-70000", "70000-80000",
            "80000-90000", "90000-100000",
        ]

    def fit(self, X: pd.DataFrame):
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        postal_codes = X["postal_code"].astype(int)
        X["postal_code_bin"] = pd.cut(postal_codes,
                                      bins=self.bins,
                                      labels=self.labels
                               )
        return X
</code></pre>
                    <p>In the context of creating custom transformers in scikit-learn, having both a <incode>fit
                        </incode> and a <incode>transform</incode> function are necessary to maintain to the
                        interface expected by the scikit-learn pipeline. Even if the transformer doesn't require
                        fitting, the presence of these methods ensures compatibility with the pipeline's
                        operation.</p>
                    <p><strong>Initialization (__init__ method)</strong><br>
                        The <incode>__init__</incode> method is used to initialize the transformer</incode>. In
                        the
                        <incode>PostalCodeBinTransformer, it defines the bins and labels that will be used to
                            categorize
                            postal codes.
                    </p>
                    <p><strong>Fitting (<incode>fit</incode> method)</strong><br>
                        The <incode>fit</incode> method is intended to prepare the transformer based on the
                        training data. However, for the <incode>PostalCodeBinTransformer</incode>, no
                        preparation based on the
                        data is needed, so the method simply returns self.<br>
                        Even though the <incode>fit</incode> method doesn't do anything in this transformer, it
                        is required because scikit-learn expects every transformer to have this method. This
                        ensures the transformer can be integrated seamlessly into a pipeline, where
                        <incode>fit</incode> will be called before <incode>transform</incode>.
                    </p>
                    <p><strong>Transformation (<incode>transform</incode> method)</strong><br>
                        The <incode>transform</incode> method is where the actual data transformation happens.
                        For the <incode>PostalCodeBinTransformer</incode>, this involves binning the postal
                        codes into predefined groups.<br>
                        In the <incode>transform</incode> method, the following happens:
                    <ul>
                        <li>

                            Conversion to integer: The postal codes are converted to integers to ensure they can
                            be properly binned.
                        </li>
                        <li>
                            Binning: The <incode>pd.cut</incode> function is used to bin the postal codes into
                            the specified
                            ranges (bins).
                        </li>
                        <li> Adding a new column: A new column, <incode>postal_code_bin</incode>, is added to
                            the DataFrame to
                            store the bin labels.</li>
                    </ul>
                    </p>
                    <p>By defining both the <incode>fit</incode> and the <incode>transform</incode> method, the
                        <incode>PostalCodeBinTransformer</incode> becomes a well-formed scikit-learn
                        transformer, ready to be used in a preprocessing pipeline. This adherence to the
                        expected interface allows for seamless integration and consistent behavior within the
                        scikit-learn framework.
                    </p>
                    <h4>
                        3. One-Hot Encoding Categorical Columns
                    </h4>
                    <p></p>
                    <pre><code class="language-python">("OneHotEncoder", one_hot_encoder)</code></pre>
                    <p>This step uses our custom transformer <incode>one_hot_encoder</incode>, which one-hot
                        encodes the specified categorical columns (<incode>job_title</incode>, <incode>city
                        </incode> and <incode>postal_code_bin</incode>). One-hot encoding converts categorical
                        variables into a form that can be provided to machine learning algorithms to do a better
                        job in
                        prediction.</p>
                    </ol>
                    <h3>Applying the Pipeline</h3>
                    <p>After defining the pipeline, we can apply it to our data to preprocess it:</p>
                    <p></p>
                    <pre><code class="language-python"># Copying our raw_data and imputing missing values with the column's mean
imputed_data = raw_data.copy()
imputed_data[cols_to_impute] = pipeline_num.fit_transform(raw_data[cols_to_impute])

# Remove the special characters from the job_title column
# Add column postal_code_bin based on the postal_code column
# One-not encode the columns in the list columns_to_encode
cleaned_data = pipeline_cat.fit_transform(imputed_data)
</code></pre>
                    <p>We first create a copy of the raw data and then apply the numerical data preprocessing
                        pipeline (<incode>pipeline_num</incode>) to impute missing values and scale the specified
                        numerical columns.</p>

                    <p>Next, we apply the categorical data preprocessing pipeline (<incode>pipeline_cat</incode>) to
                        the imputed data. This step removes special characters from the <incode>job_title</incode>
                        column, bins the postal codes, and one-hot encodes the categorical columns.</p>

                    <p>To get a better understanding of what happens before one-hot encoding, here is an
                        intermediate state of the data showing the state before one-hot encoding:
                        <br><br>
                        <img class="img-fluid"
                            src="/assets/img/posts/2024/preprocessing/preprocessing-with-scikit-learn-pipelines-part2-data-before-ohe.png">
                        <br>
                    <ol>
                        <li>
                            the column <incode>job_title</incode> no longer contains any special characters</li>
                        <li>an additional column <incode>postal_code_bin</incode> has been added, which we can use
                            for our one-hot
                            encoding in the next step</li>
                    </ol>
                    </p>
                    <p>After running our code and looking at our transformed data, everything looks fine:</p>
                    <img class="img-fluid"
                        src="/assets/img/posts/2024/preprocessing/preprocessing-with-scikit-learn-pipelines-part2-data-final.png">
                    <br>
                    <p>As the one-hot-encoded columns for the job titles and statuses are not visible in the
                        screenshot above, here are two more screenshots for the sake of completeness:</p>
                    <img class="img-fluid"
                        src="/assets/img/posts/2024/preprocessing/preprocessing-with-scikit-learn-pipelines-part2-data-job-title.png">
                    <span class="annotation">One-hot-encoded values of the job_title column.</span>
                    <br><br>
                    <img class="img-fluid"
                        src="/assets/img/posts/2024/preprocessing/preprocessing-with-scikit-learn-pipelines-part2-data-city.png">
                    <span class="annotation">One-hot-encoded values of the city column.</span>
                    <br>
                    <p>By integrating these preprocessing steps into a single pipeline, we streamline the data
                        cleaning and transformation process, making it more efficient and less prone to error. This
                        setup also ensures that our data is consistently processed in the same way every time, which
                        is critical for reproducible and reliable machine learning workflows.</p>
                    <h3>Conclusion</h3>
                    <p>In this part of the series, we have extended our preprocessing pipeline to handle categorical
                        data by removing special characters, binning postal codes, and applying one-hot encoding.
                        These steps are crucial for preparing data for machine learning models, such as neural
                        networks, which require numerical input. Stay tuned for the third and final part of
                        this series, where we will introduce scikit-learn's <incode>ColumnTransformer</incode> for a
                        more
                        elegant and robust approach to preprocessing mixed data types.</p>
                </div>
            </div>
        </div>
    </article>
    <!-- Footer-->
    <footer class="border-top">
        <div class="container px-4 px-lg-5">
            <div class="row gx-4 gx-lg-5 justify-content-center">
                <div class="col-md-10 col-lg-8 col-xl-7">
                    <ul class="list-inline text-center">
                        <li class="list-inline-item">
                            <a class="logo" href="https://www.linkedin.com/in/fabian-hruby-36b932100/">
                                <span class="fa-stack fa-lg">
                                    <i class="fas fa-circle fa-stack-2x"></i>
                                    <i class="fab fa-linkedin fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>
                        <li class="list-inline-item">
                            <a class="logo" href="https://github.com/fabianhruby">
                                <span class="fa-stack fa-lg">
                                    <i class="fas fa-circle fa-stack-2x"></i>
                                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                                </span>
                            </a>
                        </li>T
                    </ul>
                    <div class="small text-center text-muted fst-italic">Copyright &copy; Byte-Sized Insights 2024
                    </div>
                </div>
            </div>
        </div>
    </footer>
    <!-- Bootstrap core JS-->
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js"></script>
    <!-- Core theme JS-->
    <script src="/js/scripts.js"></script>
    <script src="/js/prism.js"></script>
</body>

</html>
